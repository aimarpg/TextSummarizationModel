{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aac813a",
   "metadata": {},
   "source": [
    "### DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2eec05",
   "metadata": {},
   "source": [
    "# **TEXT SUMMARIZATION MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd032a",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f93717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: rouge_score in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (0.1.2)\n",
      "Requirement already satisfied: filelock in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: absl-py in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from nltk->rouge_score) (8.2.0)\n",
      "Requirement already satisfied: joblib in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/leire/miniconda3/envs/dl/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers rouge_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import PegasusTokenizer, PegasusForConditionalGeneration\n",
    "import ast\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50cf12",
   "metadata": {},
   "source": [
    "**DATA LOADING AND PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed5068e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape: (287114, 3)\n",
      "Test dataframe shape: (11491, 3)\n",
      "Validation dataframe shape: (13369, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/test.csv', header=None)\n",
    "val_df = pd.read_csv('../data/validation.csv', header=None)\n",
    "\n",
    "columns = ['id', 'article', 'summary']\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "val_df.columns = columns\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_article_heading(article):\n",
    "    pattern = r'By\\s*\\.\\s*.*?\\s*\\.\\s*PUBLISHED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.\\s*\\|\\s*\\.\\s*UPDATED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.'\n",
    "    cleaned_text = re.sub(pattern, '', article)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['article'] = train_df['article'].apply(clean_article_heading)\n",
    "train_df['summary'] = train_df['summary'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(clean_article_heading)\n",
    "test_df['summary'] = test_df['summary'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(clean_article_heading)\n",
    "val_df['summary'] = val_df['summary'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Training dataframe shape: {train_df.shape}\")\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")\n",
    "print(f\"Validation dataframe shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b0a7fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-cnn_dailymail').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c5a24",
   "metadata": {},
   "source": [
    "**Amount reduction for training time optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd28a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(n=20000, random_state=42)\n",
    "test_df = test_df.sample(n=2000, random_state=42)\n",
    "val_df = val_df.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767ea80",
   "metadata": {},
   "source": [
    "**CUSTOM DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d3f8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, max_length=512):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.articles[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        article_encoding = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'article_input_ids': article_encoding['input_ids'].flatten(),\n",
    "            'article_attention_mask': article_encoding['attention_mask'].flatten(),\n",
    "            'summary_input_ids': summary_encoding['input_ids'].flatten(),\n",
    "            'summary_attention_mask': summary_encoding['attention_mask'].flatten()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e5fe809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 20000\n",
      "Validation samples: 2000\n",
      "Testing samples: 2000\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = NewsDataset(\n",
    "    train_df['article'].values,\n",
    "    train_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = NewsDataset(\n",
    "    val_df['article'].values,\n",
    "    val_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = NewsDataset(\n",
    "    test_df['article'].values,\n",
    "    test_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a126769",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PegasusForSummarization:\n",
    "    def __init__(self, model_name=\"google/pegasus-cnn_dailymail\", device='cuda'):\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(model_name)\n",
    "        self.model = PegasusForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "        self.device = device\n",
    "        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        \n",
    "    def train(self, train_loader, val_loader, epochs=3, learning_rate=5e-5, warmup_steps=500, weight_decay=0.01):\n",
    "        # Set up optimizer\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        # Set up scheduler\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, \n",
    "            num_warmup_steps=warmup_steps, \n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model = None\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Training]')\n",
    "            \n",
    "            for batch in train_progress_bar:\n",
    "                # Move batch to device\n",
    "                input_ids = batch['article_input_ids'].to(self.device)\n",
    "                attention_mask = batch['article_attention_mask'].to(self.device)\n",
    "                labels = batch['summary_input_ids'].to(self.device)\n",
    "                decoder_attention_mask = batch['summary_attention_mask'].to(self.device)\n",
    "                \n",
    "                # Clear gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    decoder_attention_mask=decoder_attention_mask\n",
    "                )\n",
    "                \n",
    "                loss = outputs.loss\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Update parameters\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                \n",
    "                # Update progress bar\n",
    "                train_progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, rouge_scores = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "            print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"  Rouge1: {rouge_scores['rouge1']:.4f}\")\n",
    "            print(f\"  Rouge2: {rouge_scores['rouge2']:.4f}\")\n",
    "            print(f\"  RougeL: {rouge_scores['rougeL']:.4f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "                print(f\"  New best model saved with validation loss: {val_loss:.4f}\")\n",
    "        \n",
    "        # Load best model\n",
    "        if best_model is not None:\n",
    "            self.model.load_state_dict(best_model)\n",
    "            print(f\"Loaded best model with validation loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    def evaluate(self, data_loader, max_length=128, num_beams=4):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['article_input_ids'].to(self.device)\n",
    "                attention_mask = batch['article_attention_mask'].to(self.device)\n",
    "                labels = batch['summary_input_ids'].to(self.device)\n",
    "                decoder_attention_mask = batch['summary_attention_mask'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                    decoder_attention_mask=decoder_attention_mask\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "                \n",
    "                # Generate summaries\n",
    "                generated_ids = self.model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=max_length,\n",
    "                    num_beams=num_beams,\n",
    "                    repetition_penalty=2.5,\n",
    "                    length_penalty=1.0,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "                \n",
    "                # Decode generated summaries and reference summaries\n",
    "                preds = [self.tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
    "                targets = [self.tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in labels]\n",
    "                \n",
    "                all_preds.extend(preds)\n",
    "                all_targets.extend(targets)\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(data_loader)\n",
    "        \n",
    "        # Calculate ROUGE scores\n",
    "        rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "        for pred, target in zip(all_preds, all_targets):\n",
    "            scores = self.rouge_scorer.score(target, pred)\n",
    "            rouge_scores['rouge1'] += scores['rouge1'].fmeasure\n",
    "            rouge_scores['rouge2'] += scores['rouge2'].fmeasure\n",
    "            rouge_scores['rougeL'] += scores['rougeL'].fmeasure\n",
    "        \n",
    "        # Calculate average ROUGE scores\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key] /= len(all_preds)\n",
    "        \n",
    "        return avg_val_loss, rouge_scores\n",
    "    \n",
    "    def predict(self, article, max_length=128, num_beams=4):\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Preprocess article\n",
    "        article = preprocess_text(article)\n",
    "        article = clean_article_heading(article)\n",
    "        \n",
    "        # Tokenize article\n",
    "        inputs = self.tokenizer(\n",
    "            article,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate summary\n",
    "        with torch.no_grad():\n",
    "            generated_ids = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                num_beams=num_beams,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        # Decode summary\n",
    "        summary = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "        }, path)\n",
    "        self.tokenizer.save_pretrained(path + \"_tokenizer\")\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        checkpoint = torch.load(path)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.tokenizer = PegasusTokenizer.from_pretrained(path + \"_tokenizer\")\n",
    "        print(f\"Model loaded from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df83608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['article_input_ids'].to(device)\n",
    "        attention_mask = batch['article_attention_mask'].to(device)\n",
    "        labels = batch['summary_input_ids'].to(device)\n",
    "        decoder_attention_mask = batch['summary_attention_mask'].to(device)\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "            decoder_attention_mask=decoder_attention_mask\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['article_input_ids'].to(device)\n",
    "            attention_mask = batch['article_attention_mask'].to(device)\n",
    "            labels = batch['summary_input_ids'].to(device)\n",
    "            decoder_attention_mask = batch['summary_attention_mask'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "                decoder_attention_mask=decoder_attention_mask\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Generate summaries\n",
    "            generated_ids = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=128,\n",
    "                num_beams=4,\n",
    "                repetition_penalty=2.5,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated_ids]\n",
    "            decoded_labels = [tokenizer.decode(t, skip_special_tokens=True) for t in labels]\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            for pred, label in zip(decoded_preds, decoded_labels):\n",
    "                scores = scorer.score(label, pred)\n",
    "                for metric in rouge_scores:\n",
    "                    rouge_scores[metric].append(scores[metric].fmeasure)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_rouge_scores = {k: sum(v)/len(v) for k, v in rouge_scores.items()}\n",
    "    \n",
    "    return avg_loss, avg_rouge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2a3175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NUM_EPOCHS = 3\\nLEARNING_RATE = 3e-5\\nEARLY_STOPPING_PATIENCE = 2\\n\\n# Initialize optimizer\\noptimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\\n\\n# Training loop\\nbest_val_loss = float(\\'inf\\')\\nearly_stop_counter = 0\\ntraining_stats = []\\n\\nprint(\"Starting training...\")\\n\\nfor epoch in range(NUM_EPOCHS):\\n    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\\n    \\n    train_loss = train_epoch(model, train_loader, optimizer, device)\\n    val_loss, rouge_scores = evaluate(model, val_loader, device)\\n    \\n    print(f\"Train Loss: {train_loss:.4f}\")\\n    print(f\"Val Loss: {val_loss:.4f}\")\\n    print(f\"Rouge1: {rouge_scores[\\'rouge1\\']:.4f}\")\\n    print(f\"Rouge2: {rouge_scores[\\'rouge2\\']:.4f}\")\\n    print(f\"RougeL: {rouge_scores[\\'rougeL\\']:.4f}\")\\n    \\n    # Early stopping\\n    if val_loss < best_val_loss:\\n        best_val_loss = val_loss\\n        early_stop_counter = 0\\n        torch.save(model.state_dict(), \\'best_pegasus_model.pth\\')\\n        print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\\n    else:\\n        early_stop_counter += 1\\n        print(f\"Early stopping counter: {early_stop_counter}/{EARLY_STOPPING_PATIENCE}\")\\n        \\n    if early_stop_counter >= EARLY_STOPPING_PATIENCE:\\n        print(\"Early stopping triggered!\")\\n        break\\n\\nprint(\"Training completed!\")'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training parameters\n",
    "'''NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0\n",
    "training_stats = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, rouge_scores = evaluate(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Rouge1: {rouge_scores['rouge1']:.4f}\")\n",
    "    print(f\"Rouge2: {rouge_scores['rouge2']:.4f}\")\n",
    "    print(f\"RougeL: {rouge_scores['rougeL']:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_pegasus_model.pth')\n",
    "        print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "        print(f\"Early stopping counter: {early_stop_counter}/{EARLY_STOPPING_PATIENCE}\")\n",
    "        \n",
    "    if early_stop_counter >= EARLY_STOPPING_PATIENCE:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "print(\"Training completed!\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e07e29a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, article, tokenizer, device, max_length=128):\n",
    "    model.eval()\n",
    "    \n",
    "    # Preprocess\n",
    "    article = preprocess_text(article)\n",
    "    article = clean_article_heading(article)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        article,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            repetition_penalty=2.5,\n",
    "            length_penalty=1.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf2804",
   "metadata": {},
   "source": [
    "We have had some complications with the execution code of the Pegasus model. Due to a badly performed commit, the execution of the training process was not properly uploaded to git, only an older version of the notebook, so the actual execution results per epoch are not available. \n",
    "\n",
    "Since all the model presented here were run on the open labs of the university, in order to be able to access a GPU to run the models, we had not way to recover those results neither the time to rerun the model itself. \n",
    "\n",
    "By looking at the code, it can be seen that the pegasus notebook shows an error instead of the results. We actually got that error before doing it right and fixed it by changing the batch size to 8 instead of 16, as the memory did not allow to have such a big batch size. \n",
    "\n",
    "At least, after the training process was completed we saved the best model (it is on the google folder), we could import it and perform a example summary generation. Performing an evaluation to obtain results was not tangible due to the amount of time it requires.\n",
    "This is the summary generation with the pegasus model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5b7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Summary Generation:\n",
      "Original article excerpt: David Rylance has been jailed for stealing more than £50,000 from his dying mother, who was suffering from Alzheimer's . A trusted son has been jailed for stealing more than £50,000 from his dying mot ...\n",
      "\n",
      "Generated Summary: David Rylance, 47, stole more than £50,000 from his dying mother Margaret . She was suffering from Alzheimer's and feared her money was going missing . But in reality the pensioner's son had been slowly siphoning it away . He spent it on gambling, a holiday and cinema tickets as well as living costs . Rylance admitted theft and fraud and was jailed for two years and three months .\n",
      "\n",
      "Actual Summary: David Rylance, 47, stole thousands of pounds from his own dying mother . Dying pensioner Margaret Rylance was suffering from Alzheimer's disease . She noticed money was missing but concerns were put down to condition . Her son was jailed for two years and three months for stealing £52,000 .\n"
     ]
    }
   ],
   "source": [
    "# Generate example summary\n",
    "sample_article = test_df['article'].iloc[0]\n",
    "generated_summary = generate_summary(model, sample_article, tokenizer, device)\n",
    "\n",
    "print(\"\\nSample Summary Generation:\")\n",
    "print(\"Original article excerpt:\", sample_article[:200], \"...\")\n",
    "print(\"\\nGenerated Summary:\", generated_summary)\n",
    "print(\"\\nActual Summary:\", test_df['summary'].iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
