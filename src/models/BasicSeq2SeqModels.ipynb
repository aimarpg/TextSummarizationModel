{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aac813a",
   "metadata": {},
   "source": [
    "### DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2eec05",
   "metadata": {},
   "source": [
    "## **TEXT SUMMARIZATION MODEL**\n",
    "## **BASIC SEQUENCE TO SEQUENCE MODEL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd032a",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f93717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ud/miniconda3/lib/python3.12/site-packages (4.51.3)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (0.31.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ud/miniconda3/lib/python3.12/site-packages (from transformers) (4.65.0)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Using cached absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting nltk (from rouge_score)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ud/miniconda3/lib/python3.12/site-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ud/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ud/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /home/ud/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\n",
      "Collecting click (from nltk->rouge_score)\n",
      "  Downloading click-8.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in /home/ud/miniconda3/lib/python3.12/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ud/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ud/miniconda3/lib/python3.12/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ud/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ud/miniconda3/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Using cached absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading click-8.2.0-py3-none-any.whl (102 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: click, absl-py, nltk, rouge_score\n",
      "Successfully installed absl-py-2.2.2 click-8.2.0 nltk-3.9.1 rouge_score-0.1.2\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers rouge_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F\n",
    "import ast\n",
    "import re\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a50cf12",
   "metadata": {},
   "source": [
    "**DATA LOADING AND PREPROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed5068e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape: (287114, 3)\n",
      "Test dataframe shape: (11491, 3)\n",
      "Validation dataframe shape: (13369, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/test.csv', header=None)\n",
    "val_df = pd.read_csv('../data/validation.csv', header=None)\n",
    "\n",
    "columns = ['id', 'article', 'summary']\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "val_df.columns = columns\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_article_heading(article):\n",
    "    pattern = r'By\\s*\\.\\s*.*?\\s*\\.\\s*PUBLISHED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.\\s*\\|\\s*\\.\\s*UPDATED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.'\n",
    "    cleaned_text = re.sub(pattern, '', article)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['article'] = train_df['article'].apply(clean_article_heading)\n",
    "train_df['summary'] = train_df['summary'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(clean_article_heading)\n",
    "test_df['summary'] = test_df['summary'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(clean_article_heading)\n",
    "val_df['summary'] = val_df['summary'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f\"Training dataframe shape: {train_df.shape}\")\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")\n",
    "print(f\"Validation dataframe shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c5a24",
   "metadata": {},
   "source": [
    "**Amount reduction for training time optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd28a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.sample(n=20000, random_state=42)\n",
    "test_df = test_df.sample(n=2000, random_state=42)\n",
    "val_df = val_df.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a12c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "023b9cf4dfeca75635957a73408fde39705c9d4f\n",
      "By . James Rush . A father-of-two left his wedding ring and watch to give to his children in case something happened to him before boarding the missing Malaysia Airlines flight as he flew out to start a dream job in Mongolia. Mechanical engineer Paul Weeks, 39, of Perth, Australia, was on the flight as he made his way to his first shift in a fly-in fly-out job. His wife Danica has revealed how he left the objects with her to give to their two boys if something was to happen to him. Father-of-two Paul Weeks was on board the missing flight MH370 as he flew out to Mongolia to start a dream job . Mrs Weeks however said on Sunday that she was praying for a miracle as she waited for news of him. She told 9News National in Perth: '(He said) \"If something should happen to me then the wedding ring should go to the first son that gets married and then the watch to the second\".' The former soldier, who was born in New Zealand, moved his young family to Perth after their home in Christchurch was devastated by earthquakes. The couple have a three-year-old son called Lincoln and an 11-month-old called Jack. Mr Weeks was believed to have got a new job with Transwest Mongolia. Mrs Weeks said before he left he took 'lots of photos' of his family. She told WA Today: 'I can’t give up hope. I would love him to walk through that door, hold him one more time ... I see him everywhere in the house.' Mr Weeks's wife Danica has said her husband left his wedding ring and watch in case something happened to him . Mrs Weeks has said she 'can't give up hope' after the Malaysia Airlines flight went missing early Saturday morning . Mr Weeks was one of 227 passengers and 12 crew members on board the Malaysia Airlines flight which went missing early Saturday morning as it made its way from Kuala Lumpur to Beijing. The manifest included 152 passengers from China, 38 from Malaysia, seven from Indonesia, six from Australia, five from India, three from the U.S., and others from Indonesia, France, New Zealand, Canada, Ukraine, Russia, Taiwan and the Netherlands. A mid-air explosion: The lack of debris could be explained by it falling into Malaysian jungle . A terrorist attack: Director of CIA has said terrorism could not be ruled out . Power failure: Possibly caused by deliberate cutting of power to communication instruments . Electronic warfare: 20 passengers on board were experts in this technology. Hijacking: Radar data indicates the plane might have made a U-turn. A pilot error: There is a chance of them in all air mysteries, claim experts . Structural failure: Possibly involving damage sustained by an accident in 2012 . Pilot suicide: There were two large jet crashes in the late 1990s caused by this . Aeronautical black hole: Plane is stranded hundreds of miles from current search area . Among those travelling on the flight was a group of eight Chinese and 12 Malaysian employees of Austin, Texas, semiconductor company Freescale, which said it was assembling 'around-the-clock support' for their families. For seasoned Australian travellers Robert Lawton, 58, and his wife, Catherine, 54, the seemingly routine takeoff of flight MH370 was the beginning of another adventure. Sharing their adventure was another 50-something Australian couple, Rodney and Mary Burrows. Neighbor Don Stokes said the trip was intended as the beginning of the 'next step in their life'. Among the family groups on board were teenage sweethearts Hadrien Wattrelos, 17, and Zhao Yan, 18, students at a French school in Beijing who were returning from the Malaysian leg of a two-week holiday along with Hadrien's mother and younger sister. In December, Zhao changed her Facebook profile photo to one of her and Hadrien. He had commented: 'Je t'aime,' followed by a heart, and she had 'liked' his comment. Some boarded the plane with more serious purposes in mind. Colleagues of Chandrika Sharma said the 50-year-old director of the Chennai chapter of an organization that works with fishermen was on her way from the southern Indian city to Mongolia for a Food and Agriculture Organization conference. More than three days after the Boeing 777 disappeared, no trace of the plane has been found in waters between Malaysia and Vietnam that have been scoured by more than 40 planes and ships from at least 10 nations. Catherine and Robert Lawton, from Brisbane, were named as one of three couples from Australia who were missing . French teenagers Zhao Yan and Hadrien Wattrelos had enrolled together at the Lycee Francais International de Pekin. Both are believed to have been on board . The plane dropped off radar less than an hour into the flight without sending out a distress signal. Authorities have said it may have attempted to turn back to Kuala Lumpur, but they expressed surprise that it would do so without informing ground control. Malaysia Airlines has said in a statement that search and rescue teams expanded their scope to the Malacca Strait between Malaysia's western coast and Indonesia's Sumatra island - the opposite side of Malaysia from the plane's last known location. To reach the strait, a busy shipping lane, the plane would have had to cross over the country, presumably within the range of radar. An earlier statement said the western coast of Malaysia was 'now the focus', but the airline subsequently said that phrase was an oversight. It did not elaborate. Civil aviation chief Azharuddin Abdul Rahman said the search remained 'on both sides' of the country.\n",
      "Paul Weeks was on the missing flight as he made his way to new job . Before leaving he had taken 'lots of photos' of his family, his wife has said . She said he left her his wedding ring and watch in case something happened . She has said she 'can't give up hope... I would love him to walk through that door'\n",
      "(20000, 3)\n",
      "(2000, 3)\n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_df['id'].iloc[1])\n",
    "print(train_df['article'].iloc[1])\n",
    "print(train_df['summary'].iloc[1])\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767ea80",
   "metadata": {},
   "source": [
    "**CUSTOM DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6d3f8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 20000\n",
      "Validation samples: 2000\n",
      "Testing samples: 2000\n"
     ]
    }
   ],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, tokenizer, max_length=512):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = str(self.articles[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        article_encoding = self.tokenizer(\n",
    "            article,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        summary_encoding = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=128,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'article_input_ids': article_encoding['input_ids'].flatten(),\n",
    "            'article_attention_mask': article_encoding['attention_mask'].flatten(),\n",
    "            'summary_input_ids': summary_encoding['input_ids'].flatten(),\n",
    "            'summary_attention_mask': summary_encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "train_dataset = NewsDataset(\n",
    "    train_df['article'].values,\n",
    "    train_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset = NewsDataset(\n",
    "    val_df['article'].values,\n",
    "    val_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = NewsDataset(\n",
    "    test_df['article'].values,\n",
    "    test_df['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f82baa",
   "metadata": {},
   "source": [
    "### **BASIC MODEL USING LSTMs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8b03b6",
   "metadata": {},
   "source": [
    "**MODEL DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2485f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # Pack padded sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, src_mask.sum(1).cpu(), batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        outputs, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # Unpack sequence\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim * 2]\n",
    "        batch_size, src_len, hidden_dim = encoder_outputs.shape\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        attention = attention.masked_fill(mask == 0, float('-inf'))\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        # Modify input size to account for bidirectional encoder\n",
    "        self.lstm = nn.LSTM(hidden_dim * 2 + embed_dim, hidden_dim, n_layers, \n",
    "                           dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 3 + embed_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, cell, encoder_outputs, mask):\n",
    "        # input: [batch_size, 1]\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        # Modify hidden state handling for bidirectional encoder\n",
    "        # Take only the last n_layers entries from hidden state\n",
    "        hidden = hidden[-self.lstm.num_layers:]\n",
    "        cell = cell[-self.lstm.num_layers:]\n",
    "        \n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, (hidden, cell) = self.lstm(rnn_input, (hidden, cell))\n",
    "        \n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2SeqSummarizer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, n_layers, dropout)\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, n_layers, dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
    "        \n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_mask)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, \n",
    "                                              encoder_outputs, src_mask)\n",
    "            outputs[:, t] = output\n",
    "            \n",
    "            # Teacher forcing\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0405b1c5",
   "metadata": {},
   "source": [
    "MODEL INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef00280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = Seq2SeqSummarizer(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6880d0",
   "metadata": {},
   "source": [
    "**EPOCH AND MODEL TRAINING FUNCTION DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3411397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        article_input_ids = batch['article_input_ids'].to(device)\n",
    "        article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "        summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "        \n",
    "        outputs = model(article_input_ids, article_attention_mask, summary_input_ids)\n",
    "        \n",
    "        # Reshape outputs and target for loss calculation\n",
    "        outputs = outputs[:, 1:].contiguous().view(-1, outputs.shape[-1])\n",
    "        target = summary_input_ids[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, num_epochs=10, early_stopping_patience=3):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    training_stats = []\n",
    "    best_model_state = None\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, rouge_scores = evaluate(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(\"ROUGE Scores:\", rouge_scores)\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'rouge_scores': rouge_scores\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), 'best_summarizer_model.pth')\n",
    "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stop_counter}/{early_stopping_patience}\")\n",
    "            \n",
    "        if early_stop_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Restored best model state before returning\")\n",
    "    \n",
    "    return model, training_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db090b12",
   "metadata": {},
   "source": [
    "**MODEL EVALUATION FUNCTION DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79712799",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            article_input_ids = batch['article_input_ids'].to(device)\n",
    "            article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "            summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "            \n",
    "            outputs = model(article_input_ids, article_attention_mask, summary_input_ids, \n",
    "                          teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Calculate loss\n",
    "            outputs_flat = outputs[:, 1:].contiguous().view(-1, outputs.shape[-1])\n",
    "            target_flat = summary_input_ids[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            for pred, target in zip(predictions, summary_input_ids):\n",
    "                pred_text = tokenizer.decode(pred, skip_special_tokens=True)\n",
    "                target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
    "                scores = scorer.score(target_text, pred_text)\n",
    "                \n",
    "                for metric in rouge_scores:\n",
    "                    rouge_scores[metric].append(scores[metric].fmeasure)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_rouge_scores = {k: sum(v)/len(v) for k, v in rouge_scores.items()}\n",
    "    \n",
    "    return avg_loss, avg_rouge_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc340b07",
   "metadata": {},
   "source": [
    "**TRAINING PROCESS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8e0b331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [20:49<00:00,  2.00it/s]\n",
      "Evaluating: 100%|██████████| 250/250 [00:38<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.2370\n",
      "Val Loss: 7.3687\n",
      "ROUGE Scores: {'rouge1': 0.08583217072380325, 'rouge2': 0.005423276608806084, 'rougeL': 0.0809918850896766}\n",
      "Saved new best model with validation loss: 7.3687\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [20:49<00:00,  2.00it/s]\n",
      "Evaluating: 100%|██████████| 250/250 [00:39<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.8251\n",
      "Val Loss: 7.2356\n",
      "ROUGE Scores: {'rouge1': 0.09562297230630697, 'rouge2': 0.005657155349005023, 'rougeL': 0.08491355812301712}\n",
      "Saved new best model with validation loss: 7.2356\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2500/2500 [20:51<00:00,  2.00it/s]\n",
      "Evaluating: 100%|██████████| 250/250 [00:39<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5690\n",
      "Val Loss: 7.2119\n",
      "ROUGE Scores: {'rouge1': 0.1242642444707073, 'rouge2': 0.009971465347044135, 'rougeL': 0.10986501142064836}\n",
      "Saved new best model with validation loss: 7.2119\n",
      "Restored best model state before returning\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model, training_stats = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2207e6db",
   "metadata": {},
   "source": [
    "**FINAL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f28caf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation using test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 250/250 [00:39<00:00,  6.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test results\n",
      "rouge1: 0.126447\n",
      "rouge2: 0.010083\n",
      "rougeL: 0.111677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final evaluation using test data\")\n",
    "test_loss, test_rouge_scores = evaluate(model, test_loader, criterion, device)\n",
    "print(\"Final test results\")\n",
    "for metric, score in test_rouge_scores.items():\n",
    "    print(f\"{metric}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1838da2f",
   "metadata": {},
   "source": [
    "**DATA AND BATCH SIZE AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0072fd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape: (287114, 3)\n",
      "Test dataframe shape: (11491, 3)\n",
      "Validation dataframe shape: (13369, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/test.csv', header=None)\n",
    "val_df = pd.read_csv('../data/validation.csv', header=None)\n",
    "\n",
    "columns = ['id', 'article', 'summary']\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "val_df.columns = columns\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_article_heading(article):\n",
    "    pattern = r'By\\s*\\.\\s*.*?\\s*\\.\\s*PUBLISHED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.\\s*\\|\\s*\\.\\s*UPDATED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.'\n",
    "    cleaned_text = re.sub(pattern, '', article)\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['article'] = train_df['article'].apply(clean_article_heading)\n",
    "train_df['summary'] = train_df['summary'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(clean_article_heading)\n",
    "test_df['summary'] = test_df['summary'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(clean_article_heading)\n",
    "val_df['summary'] = val_df['summary'].apply(preprocess_text)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(f\"Training dataframe shape: {train_df.shape}\")\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")\n",
    "print(f\"Validation dataframe shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68eb1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_40K = train_df.sample(n=40000, random_state=42)\n",
    "test_df_4K = test_df.sample(n=4000, random_state=42)\n",
    "val_df_4K = val_df.sample(n=4000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38ab52e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40000\n",
      "Validation samples: 4000\n",
      "Testing samples: 4000\n"
     ]
    }
   ],
   "source": [
    "train_dataset_40K = NewsDataset(\n",
    "    train_df_40K['article'].values,\n",
    "    train_df_40K['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "val_dataset_40K = NewsDataset(\n",
    "    val_df_4K['article'].values,\n",
    "    val_df_4K['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_dataset_40K = NewsDataset(\n",
    "    test_df_4K['article'].values,\n",
    "    test_df_4K['summary'].values,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader_40K = DataLoader(train_dataset_40K, batch_size=32, shuffle=True)\n",
    "val_loader_40K = DataLoader(val_dataset_40K, batch_size=32, shuffle=False)\n",
    "test_loader_40K = DataLoader(test_dataset_40K, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset_40K)}\")\n",
    "print(f\"Validation samples: {len(val_dataset_40K)}\")\n",
    "print(f\"Testing samples: {len(test_dataset_40K)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8363dcb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [22:08<00:00,  1.06s/it]\n",
      "Evaluating: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 7.0724\n",
      "Val Loss: 7.1676\n",
      "ROUGE Scores: {'rouge1': 0.1023710154628591, 'rouge2': 0.007884151610780437, 'rougeL': 0.09033332421922521}\n",
      "Saved new best model with validation loss: 7.1676\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [22:06<00:00,  1.06s/it]\n",
      "Evaluating: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.5658\n",
      "Val Loss: 7.0702\n",
      "ROUGE Scores: {'rouge1': 0.12753616370214887, 'rouge2': 0.010722532616191188, 'rougeL': 0.10685668863558208}\n",
      "Saved new best model with validation loss: 7.0702\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [22:06<00:00,  1.06s/it]\n",
      "Evaluating: 100%|██████████| 125/125 [00:56<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.2682\n",
      "Val Loss: 7.0101\n",
      "ROUGE Scores: {'rouge1': 0.12279514189694105, 'rouge2': 0.012776049571780575, 'rougeL': 0.10488697211843771}\n",
      "Saved new best model with validation loss: 7.0101\n",
      "Restored best model state before returning\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model_40K = Seq2SeqSummarizer(VOCAB_SIZE, EMBED_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model_40K.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model_40K, training_stats = train_model(\n",
    "    model=model_40K,\n",
    "    train_loader=train_loader_40K,\n",
    "    val_loader=val_loader_40K,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57f25467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation using test data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 125/125 [00:56<00:00,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test results\n",
      "rouge1: 0.123692\n",
      "rouge2: 0.012524\n",
      "rougeL: 0.106011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Final evaluation using test data\")\n",
    "test_loss, test_rouge_scores = evaluate(model_40K, test_loader_40K, criterion, device)\n",
    "print(\"Final test results\")\n",
    "for metric, score in test_rouge_scores.items():\n",
    "    print(f\"{metric}: {score:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86f6d46",
   "metadata": {},
   "source": [
    "### **BASIC MODEL USING GRUs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's add the necessary imports\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "def load_glove_embeddings(path='../data/glove.6B.300d.txt'):\n",
    "    word2vec = {}\n",
    "    embedding_dim = 300\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vector\n",
    "    return word2vec, embedding_dim\n",
    "\n",
    "# Text preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Modified Dataset class to use GloVe embeddings\n",
    "class NewsDatasetGloVe(Dataset):\n",
    "    def __init__(self, articles, summaries, word2vec, max_length=512):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.word2vec = word2vec\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = len(next(iter(word2vec.values())))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def text_to_vector(self, text, max_len):\n",
    "        words = text.split()[:max_len]\n",
    "        vectors = []\n",
    "        for word in words:\n",
    "            if word in self.word2vec:\n",
    "                vectors.append(self.word2vec[word])\n",
    "            else:\n",
    "                vectors.append(np.zeros(self.embedding_dim))\n",
    "        \n",
    "        # Pad if necessary\n",
    "        while len(vectors) < max_len:\n",
    "            vectors.append(np.zeros(self.embedding_dim))\n",
    "        \n",
    "        return torch.tensor(vectors, dtype=torch.float32)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = preprocess_text(str(self.articles[idx]))\n",
    "        summary = preprocess_text(str(self.summaries[idx]))\n",
    "        \n",
    "        article_vector = self.text_to_vector(article, self.max_length)\n",
    "        summary_vector = self.text_to_vector(summary, 128)\n",
    "        \n",
    "        return {\n",
    "            'article_vector': article_vector,\n",
    "            'summary_vector': summary_vector\n",
    "        }\n",
    "\n",
    "# GRU-based Encoder\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers,\n",
    "                         dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len, embedding_dim]\n",
    "        outputs, hidden = self.gru(self.dropout(src))\n",
    "        return outputs, hidden\n",
    "\n",
    "# GRU-based Decoder with Attention\n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.gru = nn.GRU(hidden_dim * 2 + embed_dim, hidden_dim, n_layers,\n",
    "                         dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 3 + embed_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        hidden = hidden[-self.gru.num_layers:]\n",
    "        \n",
    "        a = self.attention(hidden[-1], encoder_outputs, mask)\n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim=2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "        \n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))\n",
    "        return prediction, hidden\n",
    "\n",
    "# Seq2Seq model with GRU\n",
    "class Seq2SeqGRU(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs,\n",
    "                                        torch.ones_like(src[:,:,0]))\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1) < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "# Initialize model with GloVe embeddings\n",
    "print(\"Loading GloVe embeddings...\")\n",
    "word2vec, EMBEDDING_DIM = load_glove_embeddings()\n",
    "\n",
    "# Create datasets with GloVe embeddings\n",
    "train_dataset_gru = NewsDatasetGloVe(\n",
    "    train_df['article'].values,\n",
    "    train_df['summary'].values,\n",
    "    word2vec\n",
    ")\n",
    "\n",
    "val_dataset_gru = NewsDatasetGloVe(\n",
    "    val_df['article'].values,\n",
    "    val_df['summary'].values,\n",
    "    word2vec\n",
    ")\n",
    "\n",
    "test_dataset_gru = NewsDatasetGloVe(\n",
    "    test_df['article'].values,\n",
    "    test_df['summary'].values,\n",
    "    word2vec\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader_gru = DataLoader(train_dataset_gru, batch_size=8, shuffle=True)\n",
    "val_loader_gru = DataLoader(val_dataset_gru, batch_size=8, shuffle=False)\n",
    "test_loader_gru = DataLoader(test_dataset_gru, batch_size=8, shuffle=False)\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_DIM = EMBEDDING_DIM\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Initialize the model\n",
    "encoder = GRUEncoder(INPUT_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = GRUDecoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model_gru = Seq2SeqGRU(encoder, decoder, device).to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer_gru = optim.Adam(model_gru.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training GRU model...\")\n",
    "model_gru, training_stats_gru = train_model(\n",
    "    model=model_gru,\n",
    "    train_loader=train_loader_gru,\n",
    "    val_loader=val_loader_gru,\n",
    "    optimizer=optimizer_gru,\n",
    "    criterion=criterion,\n",
    "    device=device,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nFinal evaluation using test data\")\n",
    "test_loss, test_rouge_scores = evaluate(model_gru, test_loader_gru, criterion, device)\n",
    "print(\"\\nFinal test results for GRU model:\")\n",
    "for metric, score in test_rouge_scores.items():\n",
    "    print(f\"{metric}: {score:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
