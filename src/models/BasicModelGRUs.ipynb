{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2161db",
   "metadata": {},
   "source": [
    "### **BASIC MODEL USING GRUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3bd99",
   "metadata": {},
   "source": [
    "**Custom dataset definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb1d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BertTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_word2vec_embeddings(path='../data/GoogleNews-vectors-negative300.bin'):\n",
    "    print(\"Loading Word2Vec embeddings...\")\n",
    "    word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "    embedding_dim = word2vec.vector_size\n",
    "    return word2vec, embedding_dim\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_article_heading(article):\n",
    "    pattern = r'By\\s*\\.\\s*.*?\\s*\\.\\s*PUBLISHED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.\\s*\\|\\s*\\.\\s*UPDATED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.'\n",
    "    cleaned_text = re.sub(pattern, '', article)\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e97868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape: (287114, 3)\n",
      "Test dataframe shape: (11491, 3)\n",
      "Validation dataframe shape: (13369, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/test.csv', header=None)\n",
    "val_df = pd.read_csv('../data/validation.csv', header=None)\n",
    "\n",
    "columns = ['id', 'article', 'summary']\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "val_df.columns = columns\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['article'] = train_df['article'].apply(clean_article_heading)\n",
    "train_df['summary'] = train_df['summary'].apply(preprocess_text)\n",
    "\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(clean_article_heading)\n",
    "test_df['summary'] = test_df['summary'].apply(preprocess_text)\n",
    "\n",
    "val_df['article'] = val_df['article'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(clean_article_heading)\n",
    "val_df['summary'] = val_df['summary'].apply(preprocess_text)\n",
    "\n",
    "print(f\"Training dataframe shape: {train_df.shape}\")\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")\n",
    "print(f\"Validation dataframe shape: {val_df.shape}\")\n",
    "\n",
    "train_df = train_df.sample(n=20000, random_state=42)\n",
    "test_df = test_df.sample(n=2000, random_state=42)\n",
    "val_df = val_df.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce8f01",
   "metadata": {},
   "source": [
    "**CUSTOM DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36b904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for summarization task with BERT tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, articles, summaries, tokenizer_wrapper, max_article_len=512, max_summary_len=128):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer_wrapper = tokenizer_wrapper\n",
    "        self.max_article_len = max_article_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        \n",
    "        # Convert to sequences\n",
    "        article_seq = self.tokenizer_wrapper.text_to_sequence(article, self.max_article_len)\n",
    "        summary_seq = self.tokenizer_wrapper.text_to_sequence(summary, self.max_summary_len)\n",
    "        \n",
    "        # Add SOS and EOS tokens to summary\n",
    "        sos_idx = self.tokenizer_wrapper.vocab_to_int['[SOS]']\n",
    "        eos_idx = self.tokenizer_wrapper.vocab_to_int['[EOS]']\n",
    "        summary_with_tokens = [sos_idx] + summary_seq + [eos_idx]\n",
    "        \n",
    "        return {\n",
    "            'article_input_ids': torch.tensor(article_seq, dtype=torch.long),\n",
    "            'article_attention_mask': torch.ones(len(article_seq), dtype=torch.long),\n",
    "            'summary_input_ids': torch.tensor(summary_with_tokens, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    # Extract sequences\n",
    "    article_input_ids = [item['article_input_ids'] for item in batch]\n",
    "    article_attention_masks = [item['article_attention_mask'] for item in batch]\n",
    "    summary_input_ids = [item['summary_input_ids'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    article_input_ids_padded = pad_sequence(article_input_ids, batch_first=True, padding_value=0)\n",
    "    article_attention_masks_padded = pad_sequence(article_attention_masks, batch_first=True, padding_value=0)\n",
    "    summary_input_ids_padded = pad_sequence(summary_input_ids, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'article_input_ids': article_input_ids_padded,\n",
    "        'article_attention_mask': article_attention_masks_padded,\n",
    "        'summary_input_ids': summary_input_ids_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43712d",
   "metadata": {},
   "source": [
    "**SENTENCE TOKENIZATION AND WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace5e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper:\n",
    "    \"\"\"Wrapper to handle BERT tokenizer with custom vocabulary for Word2Vec\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', max_vocab_size=10000):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.word_to_subwords = {}\n",
    "        \n",
    "    def build_vocabulary_from_word2vec(self, word2vec_model, texts):\n",
    "        \"\"\"Build vocabulary from Word2Vec model and training texts\"\"\"\n",
    "        # Start with special tokens\n",
    "        special_tokens = ['[PAD]', '[UNK]', '[SOS]', '[EOS]']\n",
    "        self.vocab_to_int = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        self.int_to_vocab = {idx: token for idx, token in enumerate(special_tokens)}\n",
    "        \n",
    "        # Extract words from texts and filter by Word2Vec vocabulary\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            # Use BERT tokenizer to get subwords, then extract unique words\n",
    "            tokens = self.tokenizer.tokenize(text.lower())\n",
    "            # Convert subwords back to words for Word2Vec lookup\n",
    "            words = []\n",
    "            current_word = \"\"\n",
    "            for token in tokens:\n",
    "                if token.startswith('##'):\n",
    "                    current_word += token[2:]\n",
    "                else:\n",
    "                    if current_word:\n",
    "                        words.append(current_word)\n",
    "                    current_word = token\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            \n",
    "            # Only count words that exist in Word2Vec\n",
    "            valid_words = [word for word in words if word in word2vec_model.key_to_index]\n",
    "            word_freq.update(valid_words)\n",
    "        \n",
    "        # Sort by frequency and add to vocabulary\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_words = [word for word, freq in sorted_words[:self.max_vocab_size - len(special_tokens)]]\n",
    "        \n",
    "        for word in vocab_words:\n",
    "            idx = len(self.vocab_to_int)\n",
    "            self.vocab_to_int[word] = idx\n",
    "            self.int_to_vocab[idx] = word\n",
    "            # Store subword mapping\n",
    "            self.word_to_subwords[word] = self.tokenizer.tokenize(word)\n",
    "        \n",
    "        print(f\"Built vocabulary with {len(self.vocab_to_int)} tokens\")\n",
    "        print(f\"Words from Word2Vec: {len(vocab_words)}\")\n",
    "    \n",
    "    def text_to_sequence(self, text, max_length=512):\n",
    "        \"\"\"Convert text to sequence of word indices\"\"\"\n",
    "        # Tokenize with BERT\n",
    "        subword_tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert subwords back to words\n",
    "        words = []\n",
    "        current_word = \"\"\n",
    "        for token in subword_tokens:\n",
    "            if token.startswith('##'):\n",
    "                current_word += token[2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    words.append(current_word)\n",
    "                current_word = token\n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "        \n",
    "        # Convert to indices\n",
    "        sequence = []\n",
    "        for word in words[:max_length]:\n",
    "            if word in self.vocab_to_int:\n",
    "                sequence.append(self.vocab_to_int[word])\n",
    "            else:\n",
    "                sequence.append(self.vocab_to_int['[UNK]'])\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence):\n",
    "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            if isinstance(idx, torch.Tensor):\n",
    "                idx = idx.item()\n",
    "            if idx in [0, 2, 3]:  # Skip PAD, SOS, EOS\n",
    "                if idx == 3:  # Stop at EOS\n",
    "                    break\n",
    "                continue\n",
    "            if idx in self.int_to_vocab:\n",
    "                words.append(self.int_to_vocab[idx])\n",
    "        return ' '.join(words)\n",
    "\n",
    "class Word2VecEmbeddings:\n",
    "    \"\"\"Word2Vec embeddings handler for custom vocabulary\"\"\"\n",
    "    \n",
    "    def __init__(self, word2vec_model, embedding_dim):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def create_embedding_matrix(self, tokenizer_wrapper):\n",
    "        \"\"\"Create embedding matrix for the custom vocabulary\"\"\"\n",
    "        vocab_size = len(tokenizer_wrapper.vocab_to_int)\n",
    "        embedding_matrix = np.random.normal(0, 0.1, (vocab_size, self.embedding_dim))\n",
    "        \n",
    "        found_words = 0\n",
    "        for word, idx in tokenizer_wrapper.vocab_to_int.items():\n",
    "            if word in ['[PAD]', '[UNK]', '[SOS]', '[EOS]']:\n",
    "                # Initialize special tokens with small random values\n",
    "                embedding_matrix[idx] = np.random.normal(0, 0.01, self.embedding_dim)\n",
    "            elif word in self.word2vec_model.key_to_index:\n",
    "                embedding_matrix[idx] = self.word2vec_model[word]\n",
    "                found_words += 1\n",
    "        \n",
    "        print(f\"Found Word2Vec embeddings for {found_words}/{vocab_size-4} words\")\n",
    "        return torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120b7f9",
   "metadata": {},
   "source": [
    "**MODEL DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a04a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder with GRU and Word2Vec embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3, embedding_matrix=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            \n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        # hidden shape: (num_layers * 2, batch_size, hidden_dim)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs, encoder_mask=None):\n",
    "        # decoder_hidden shape: (batch_size, hidden_dim)\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden state\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Concatenate and compute attention scores\n",
    "        energy = torch.cat([decoder_hidden, encoder_outputs], dim=2)\n",
    "        energy = torch.tanh(self.attn(energy))\n",
    "        attention_scores = self.v(energy).squeeze(2)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if encoder_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(encoder_mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder with attention and Word2Vec embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.3, embedding_matrix=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            \n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim * 2, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout)\n",
    "        self.output_projection = nn.Linear(hidden_dim * 3, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_token, decoder_hidden, encoder_outputs, encoder_mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        # Get attention context\n",
    "        context, attention_weights = self.attention(decoder_hidden[-1], encoder_outputs, encoder_mask)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        context = context.unsqueeze(1)\n",
    "        gru_input = torch.cat([embedded, context], dim=2)\n",
    "        \n",
    "        output, decoder_hidden = self.gru(gru_input, decoder_hidden)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction_input = torch.cat([output.squeeze(1), context.squeeze(1)], dim=1)\n",
    "        prediction = self.output_projection(prediction_input)\n",
    "        \n",
    "        return prediction, decoder_hidden, attention_weights\n",
    "\n",
    "class SummarizationModel(nn.Module):\n",
    "    \"\"\"Complete summarization model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=2, dropout=0.3, embedding_matrix=None):\n",
    "        super(SummarizationModel, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers, dropout, embedding_matrix)\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, 1, dropout, embedding_matrix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, article_input_ids, article_attention_mask, summary_input_ids=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = article_input_ids.size(0)\n",
    "        \n",
    "        # Encode article\n",
    "        encoder_outputs, encoder_hidden = self.encoder(article_input_ids, article_attention_mask)\n",
    "        \n",
    "        # Initialize decoder hidden state\n",
    "        decoder_hidden = encoder_hidden[-2:].mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if summary_input_ids is not None:\n",
    "            # Training mode\n",
    "            max_length = summary_input_ids.size(1) - 1  # Exclude last token\n",
    "            outputs = []\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                if t == 0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    # Use teacher forcing\n",
    "                    input_token = summary_input_ids[:, t:t+1]\n",
    "                else:\n",
    "                    # Use previous prediction\n",
    "                    input_token = torch.argmax(outputs[-1], dim=1, keepdim=True)\n",
    "                \n",
    "                output, decoder_hidden, _ = self.decoder(\n",
    "                    input_token, decoder_hidden, encoder_outputs, \n",
    "                    encoder_mask=article_attention_mask\n",
    "                )\n",
    "                outputs.append(output)\n",
    "            \n",
    "            return torch.stack(outputs, dim=1)\n",
    "        else:\n",
    "            # Inference mode\n",
    "            max_length = 100\n",
    "            outputs = []\n",
    "            input_token = torch.tensor([[2]], device=article_input_ids.device).repeat(batch_size, 1)  # [SOS]\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                output, decoder_hidden, _ = self.decoder(\n",
    "                    input_token, decoder_hidden, encoder_outputs,\n",
    "                    encoder_mask=article_attention_mask\n",
    "                )\n",
    "                outputs.append(output)\n",
    "                input_token = torch.argmax(output, dim=1, keepdim=True)\n",
    "                \n",
    "                # Stop if all sequences have generated [EOS]\n",
    "                if (input_token == 3).all():\n",
    "                    break\n",
    "            \n",
    "            return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fba821",
   "metadata": {},
   "source": [
    "**MODEL TRAINING PROCESS DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27085004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        article_input_ids = batch['article_input_ids'].to(device)\n",
    "        article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "        summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "        \n",
    "        outputs = model(article_input_ids, article_attention_mask, summary_input_ids)\n",
    "        \n",
    "        # Get target sequence excluding the last token and SOS token\n",
    "        target = summary_input_ids[:, 1:-1].contiguous()\n",
    "        \n",
    "        # Ensure outputs match target sequence length\n",
    "        outputs = outputs[:, :target.size(1), :].contiguous()\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, tokenizer_wrapper):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            article_input_ids = batch['article_input_ids'].to(device)\n",
    "            article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "            summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "            \n",
    "            outputs = model(article_input_ids, article_attention_mask, summary_input_ids,\n",
    "                          teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Get target sequence excluding the last token and SOS token\n",
    "            target = summary_input_ids[:, 1:-1].contiguous()\n",
    "            \n",
    "            # Ensure outputs match target sequence length\n",
    "            outputs = outputs[:, :target.size(1), :].contiguous()\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            target_flat = target.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            for pred, target in zip(predictions, summary_input_ids):\n",
    "                pred_text = tokenizer_wrapper.sequence_to_text(pred)\n",
    "                target_text = tokenizer_wrapper.sequence_to_text(target)\n",
    "                \n",
    "                if pred_text.strip() and target_text.strip():\n",
    "                    scores = scorer.score(target_text, pred_text)\n",
    "                    for metric in rouge_scores:\n",
    "                        rouge_scores[metric].append(scores[metric].fmeasure)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_rouge_scores = {k: sum(v)/len(v) if v else 0.0 for k, v in rouge_scores.items()}\n",
    "    \n",
    "    return avg_loss, avg_rouge_scores\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, tokenizer_wrapper, \n",
    "                num_epochs=10, early_stopping_patience=3):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    training_stats = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, rouge_scores = evaluate(model, val_loader, criterion, device, tokenizer_wrapper)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(\"ROUGE Scores:\", rouge_scores)\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'rouge_scores': rouge_scores\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), 'best_summarizer_model.pth')\n",
    "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stop_counter}/{early_stopping_patience}\")\n",
    "            \n",
    "        if early_stop_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Restored best model state before returning\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bf13083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings...\n",
      "Building vocabulary...\n",
      "Built vocabulary with 10000 tokens\n",
      "Words from Word2Vec: 9996\n",
      "Creating embedding matrix...\n",
      "Found Word2Vec embeddings for 9996/9996 words\n"
     ]
    }
   ],
   "source": [
    "word2vec_model, embedding_dim = load_word2vec_embeddings('../data/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# Initialize BERT tokenizer wrapper\n",
    "print(\"Building vocabulary...\")\n",
    "tokenizer_wrapper = BertTokenizerWrapper(max_vocab_size=10000)\n",
    "all_texts = train_df['article'].tolist() + train_df['summary'].tolist()\n",
    "tokenizer_wrapper.build_vocabulary_from_word2vec(word2vec_model, all_texts)\n",
    "\n",
    "# Create Word2Vec embeddings for the vocabulary\n",
    "print(\"Creating embedding matrix...\")\n",
    "word2vec_embeddings = Word2VecEmbeddings(word2vec_model, embedding_dim)\n",
    "embedding_matrix = word2vec_embeddings.create_embedding_matrix(tokenizer_wrapper)\n",
    "\n",
    "train_dataset = SummarizationDataset(\n",
    "        train_df['article'].tolist(), \n",
    "        train_df['summary'].tolist(), \n",
    "        tokenizer_wrapper\n",
    "    )\n",
    "val_dataset = SummarizationDataset(\n",
    "    val_df['article'].tolist(), \n",
    "    val_df['summary'].tolist(), \n",
    "    tokenizer_wrapper\n",
    "    )\n",
    "test_dataset = SummarizationDataset(\n",
    "    test_df['article'].tolist(), \n",
    "    test_df['summary'].tolist(), \n",
    "    tokenizer_wrapper\n",
    "    )\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacb6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl/lib/python3.12/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 26/1250 [2:58:52<31:08:48, 91.61s/it]  "
     ]
    }
   ],
   "source": [
    "print(\"Initializing model...\")\n",
    "vocab_size = len(tokenizer_wrapper.vocab_to_int)\n",
    "model = SummarizationModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trained_model, training_stats = train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, tokenizer_wrapper,\n",
    "    num_epochs=15, early_stopping_patience=3\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_rouge_scores = evaluate(trained_model, test_loader, criterion, device, tokenizer_wrapper)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"Test ROUGE Scores:\", test_rouge_scores)\n",
    "\n",
    "# Save final model and tokenizer\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'training_stats': training_stats,\n",
    "    'test_scores': {'loss': test_loss, 'rouge': test_rouge_scores}\n",
    "}, 'final_summarization_model.pth')\n",
    "\n",
    "with open('tokenizer_wrapper.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_wrapper, f)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Example inference\n",
    "print(\"\\nExample inference:\")\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_idx = 0\n",
    "    sample_article = test_df.iloc[sample_idx]['article']\n",
    "    actual_summary = test_df.iloc[sample_idx]['summary']\n",
    "    \n",
    "    # Preprocess\n",
    "    article_seq = tokenizer_wrapper.text_to_sequence(sample_article)\n",
    "    article_tensor = torch.tensor([article_seq], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones_like(article_tensor).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    outputs = trained_model(article_tensor, attention_mask)\n",
    "    predicted_ids = torch.argmax(outputs, dim=-1)[0]\n",
    "    \n",
    "    # Decode\n",
    "    generated_summary = tokenizer_wrapper.sequence_to_text(predicted_ids)\n",
    "    \n",
    "    print(f\"Article: {sample_article[:200]}...\")\n",
    "    print(f\"Actual Summary: {actual_summary}\")\n",
    "    print(f\"Generated Summary: {generated_summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
