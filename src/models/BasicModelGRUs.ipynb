{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae2161db",
   "metadata": {},
   "source": [
    "### **BASIC MODEL USING GRUs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3bd99",
   "metadata": {},
   "source": [
    "**Custom dataset definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb1d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: nltk in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: rouge-score in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: transformers in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: gensim in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: tqdm in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (80.7.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from nltk) (8.2.0)\n",
      "Requirement already satisfied: joblib in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from nltk) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: absl-py in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from rouge-score) (2.2.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from rouge-score) (1.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages (from requests->transformers) (2025.4.26)\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "!pip install torch numpy pandas nltk rouge-score transformers gensim tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BertTokenizer\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# def load_word2vec_embeddings(path='../data/GoogleNews-vectors-negative300.bin'):\n",
    "#     print(\"Loading Word2Vec embeddings...\")\n",
    "#     word2vec = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "#     embedding_dim = word2vec.vector_size\n",
    "#     return word2vec, embedding_dim\n",
    "\n",
    "def load_word2vec_embeddings():\n",
    "    import gensim.downloader as api\n",
    "\n",
    "    print(\"Loading Word2Vec embeddings from gensim-data...\")\n",
    "    word2vec = api.load(\"word2vec-google-news-300\")\n",
    "    embedding_dim = word2vec.vector_size\n",
    "\n",
    "    print(f\"Word2Vec embedding dimension: {embedding_dim}\")\n",
    "    return word2vec, embedding_dim\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "def clean_article_heading(article):\n",
    "    pattern = r'By\\s*\\.\\s*.*?\\s*\\.\\s*PUBLISHED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.\\s*\\|\\s*\\.\\s*UPDATED:\\s*\\.\\s*\\d+:\\d+\\s*EST,\\s*\\d+\\s*[A-Za-z]+\\s*\\d+\\s*\\.'\n",
    "    cleaned_text = re.sub(pattern, '', article)\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1e97868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataframe shape: (287114, 3)\n",
      "Test dataframe shape: (11491, 3)\n",
      "Validation dataframe shape: (13369, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('../data/train.csv', header=None)\n",
    "test_df = pd.read_csv('../data/test.csv', header=None)\n",
    "val_df = pd.read_csv('../data/validation.csv', header=None)\n",
    "\n",
    "columns = ['id', 'article', 'summary']\n",
    "train_df.columns = columns\n",
    "test_df.columns = columns\n",
    "val_df.columns = columns\n",
    "\n",
    "train_df['article'] = train_df['article'].apply(preprocess_text)\n",
    "train_df['article'] = train_df['article'].apply(clean_article_heading)\n",
    "train_df['summary'] = train_df['summary'].apply(preprocess_text)\n",
    "\n",
    "test_df['article'] = test_df['article'].apply(preprocess_text)\n",
    "test_df['article'] = test_df['article'].apply(clean_article_heading)\n",
    "test_df['summary'] = test_df['summary'].apply(preprocess_text)\n",
    "\n",
    "val_df['article'] = val_df['article'].apply(preprocess_text)\n",
    "val_df['article'] = val_df['article'].apply(clean_article_heading)\n",
    "val_df['summary'] = val_df['summary'].apply(preprocess_text)\n",
    "\n",
    "print(f\"Training dataframe shape: {train_df.shape}\")\n",
    "print(f\"Test dataframe shape: {test_df.shape}\")\n",
    "print(f\"Validation dataframe shape: {val_df.shape}\")\n",
    "\n",
    "train_df = train_df.sample(n=20000, random_state=42)\n",
    "test_df = test_df.sample(n=2000, random_state=42)\n",
    "val_df = val_df.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ce8f01",
   "metadata": {},
   "source": [
    "**CUSTOM DATASET**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b904e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for summarization task with BERT tokenizer\"\"\"\n",
    "    \n",
    "    def __init__(self, articles, summaries, tokenizer_wrapper, max_article_len=512, max_summary_len=128):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer_wrapper = tokenizer_wrapper\n",
    "        self.max_article_len = max_article_len\n",
    "        self.max_summary_len = max_summary_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        article = self.articles[idx]\n",
    "        summary = self.summaries[idx]\n",
    "        \n",
    "        # Convert to sequences\n",
    "        article_seq = self.tokenizer_wrapper.text_to_sequence(article, self.max_article_len)\n",
    "        summary_seq = self.tokenizer_wrapper.text_to_sequence(summary, self.max_summary_len)\n",
    "        \n",
    "        # Add SOS and EOS tokens to summary\n",
    "        sos_idx = self.tokenizer_wrapper.vocab_to_int['[SOS]']\n",
    "        eos_idx = self.tokenizer_wrapper.vocab_to_int['[EOS]']\n",
    "        summary_with_tokens = [sos_idx] + summary_seq + [eos_idx]\n",
    "        \n",
    "        return {\n",
    "            'article_input_ids': torch.tensor(article_seq, dtype=torch.long),\n",
    "            'article_attention_mask': torch.ones(len(article_seq), dtype=torch.long),\n",
    "            'summary_input_ids': torch.tensor(summary_with_tokens, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    # Extract sequences\n",
    "    article_input_ids = [item['article_input_ids'] for item in batch]\n",
    "    article_attention_masks = [item['article_attention_mask'] for item in batch]\n",
    "    summary_input_ids = [item['summary_input_ids'] for item in batch]\n",
    "    \n",
    "    # Pad sequences\n",
    "    article_input_ids_padded = pad_sequence(article_input_ids, batch_first=True, padding_value=0)\n",
    "    article_attention_masks_padded = pad_sequence(article_attention_masks, batch_first=True, padding_value=0)\n",
    "    summary_input_ids_padded = pad_sequence(summary_input_ids, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'article_input_ids': article_input_ids_padded,\n",
    "        'article_attention_mask': article_attention_masks_padded,\n",
    "        'summary_input_ids': summary_input_ids_padded\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f43712d",
   "metadata": {},
   "source": [
    "**SENTENCE TOKENIZATION AND WORD EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ace5e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizerWrapper:\n",
    "    \"\"\"Wrapper to handle BERT tokenizer with custom vocabulary for Word2Vec\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='bert-base-uncased', max_vocab_size=10000):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.vocab_to_int = {}\n",
    "        self.int_to_vocab = {}\n",
    "        self.word_to_subwords = {}\n",
    "        \n",
    "    def build_vocabulary_from_word2vec(self, word2vec_model, texts):\n",
    "        \"\"\"Build vocabulary from Word2Vec model and training texts\"\"\"\n",
    "        # Start with special tokens\n",
    "        special_tokens = ['[PAD]', '[UNK]', '[SOS]', '[EOS]']\n",
    "        self.vocab_to_int = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "        self.int_to_vocab = {idx: token for idx, token in enumerate(special_tokens)}\n",
    "        \n",
    "        # Extract words from texts and filter by Word2Vec vocabulary\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            # Use BERT tokenizer to get subwords, then extract unique words\n",
    "            tokens = self.tokenizer.tokenize(text.lower())\n",
    "            # Convert subwords back to words for Word2Vec lookup\n",
    "            words = []\n",
    "            current_word = \"\"\n",
    "            for token in tokens:\n",
    "                if token.startswith('##'):\n",
    "                    current_word += token[2:]\n",
    "                else:\n",
    "                    if current_word:\n",
    "                        words.append(current_word)\n",
    "                    current_word = token\n",
    "            if current_word:\n",
    "                words.append(current_word)\n",
    "            \n",
    "            # Only count words that exist in Word2Vec\n",
    "            valid_words = [word for word in words if word in word2vec_model.key_to_index]\n",
    "            word_freq.update(valid_words)\n",
    "        \n",
    "        # Sort by frequency and add to vocabulary\n",
    "        sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        vocab_words = [word for word, freq in sorted_words[:self.max_vocab_size - len(special_tokens)]]\n",
    "        \n",
    "        for word in vocab_words:\n",
    "            idx = len(self.vocab_to_int)\n",
    "            self.vocab_to_int[word] = idx\n",
    "            self.int_to_vocab[idx] = word\n",
    "            # Store subword mapping\n",
    "            self.word_to_subwords[word] = self.tokenizer.tokenize(word)\n",
    "        \n",
    "        print(f\"Built vocabulary with {len(self.vocab_to_int)} tokens\")\n",
    "        print(f\"Words from Word2Vec: {len(vocab_words)}\")\n",
    "    \n",
    "    def text_to_sequence(self, text, max_length=512):\n",
    "        \"\"\"Convert text to sequence of word indices\"\"\"\n",
    "        # Tokenize with BERT\n",
    "        subword_tokens = self.tokenizer.tokenize(text.lower())\n",
    "        \n",
    "        # Convert subwords back to words\n",
    "        words = []\n",
    "        current_word = \"\"\n",
    "        for token in subword_tokens:\n",
    "            if token.startswith('##'):\n",
    "                current_word += token[2:]\n",
    "            else:\n",
    "                if current_word:\n",
    "                    words.append(current_word)\n",
    "                current_word = token\n",
    "        if current_word:\n",
    "            words.append(current_word)\n",
    "        \n",
    "        # Convert to indices\n",
    "        sequence = []\n",
    "        for word in words[:max_length]:\n",
    "            if word in self.vocab_to_int:\n",
    "                sequence.append(self.vocab_to_int[word])\n",
    "            else:\n",
    "                sequence.append(self.vocab_to_int['[UNK]'])\n",
    "        \n",
    "        return sequence\n",
    "    \n",
    "    def sequence_to_text(self, sequence):\n",
    "        \"\"\"Convert sequence of indices back to text\"\"\"\n",
    "        words = []\n",
    "        for idx in sequence:\n",
    "            if isinstance(idx, torch.Tensor):\n",
    "                idx = idx.item()\n",
    "            if idx in [0, 2, 3]:  # Skip PAD, SOS, EOS\n",
    "                if idx == 3:  # Stop at EOS\n",
    "                    break\n",
    "                continue\n",
    "            if idx in self.int_to_vocab:\n",
    "                words.append(self.int_to_vocab[idx])\n",
    "        return ' '.join(words)\n",
    "\n",
    "class Word2VecEmbeddings:\n",
    "    \"\"\"Word2Vec embeddings handler for custom vocabulary\"\"\"\n",
    "    \n",
    "    def __init__(self, word2vec_model, embedding_dim):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def create_embedding_matrix(self, tokenizer_wrapper):\n",
    "        \"\"\"Create embedding matrix for the custom vocabulary\"\"\"\n",
    "        vocab_size = len(tokenizer_wrapper.vocab_to_int)\n",
    "        embedding_matrix = np.random.normal(0, 0.1, (vocab_size, self.embedding_dim))\n",
    "        \n",
    "        found_words = 0\n",
    "        for word, idx in tokenizer_wrapper.vocab_to_int.items():\n",
    "            if word in ['[PAD]', '[UNK]', '[SOS]', '[EOS]']:\n",
    "                # Initialize special tokens with small random values\n",
    "                embedding_matrix[idx] = np.random.normal(0, 0.01, self.embedding_dim)\n",
    "            elif word in self.word2vec_model.key_to_index:\n",
    "                embedding_matrix[idx] = self.word2vec_model[word]\n",
    "                found_words += 1\n",
    "        \n",
    "        print(f\"Found Word2Vec embeddings for {found_words}/{vocab_size-4} words\")\n",
    "        return torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c120b7f9",
   "metadata": {},
   "source": [
    "**MODEL DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a04a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder with GRU and Word2Vec embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=2, dropout=0.3, embedding_matrix=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            \n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        # outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        # hidden shape: (num_layers * 2, batch_size, hidden_dim)\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"Attention mechanism\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs, encoder_mask=None):\n",
    "        # decoder_hidden shape: (batch_size, hidden_dim)\n",
    "        # encoder_outputs shape: (batch_size, seq_len, hidden_dim * 2)\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        seq_len = encoder_outputs.size(1)\n",
    "        \n",
    "        # Repeat decoder hidden state\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, seq_len, 1)\n",
    "        \n",
    "        # Concatenate and compute attention scores\n",
    "        energy = torch.cat([decoder_hidden, encoder_outputs], dim=2)\n",
    "        energy = torch.tanh(self.attn(energy))\n",
    "        attention_scores = self.v(energy).squeeze(2)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if encoder_mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(encoder_mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Compute context vector\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        context = context.squeeze(1)\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder with attention and Word2Vec embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.3, embedding_matrix=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Initialize embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding.weight.data.copy_(embedding_matrix)\n",
    "            \n",
    "        self.attention = Attention(hidden_dim)\n",
    "        self.gru = nn.GRU(embed_dim + hidden_dim * 2, hidden_dim, num_layers, \n",
    "                         batch_first=True, dropout=dropout)\n",
    "        self.output_projection = nn.Linear(hidden_dim * 3, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_token, decoder_hidden, encoder_outputs, encoder_mask=None):\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        \n",
    "        # Get attention context\n",
    "        context, attention_weights = self.attention(decoder_hidden[-1], encoder_outputs, encoder_mask)\n",
    "        \n",
    "        # Concatenate embedding and context\n",
    "        context = context.unsqueeze(1)\n",
    "        gru_input = torch.cat([embedded, context], dim=2)\n",
    "        \n",
    "        output, decoder_hidden = self.gru(gru_input, decoder_hidden)\n",
    "        \n",
    "        # Final prediction\n",
    "        prediction_input = torch.cat([output.squeeze(1), context.squeeze(1)], dim=1)\n",
    "        prediction = self.output_projection(prediction_input)\n",
    "        \n",
    "        return prediction, decoder_hidden, attention_weights\n",
    "\n",
    "class SummarizationModel(nn.Module):\n",
    "    \"\"\"Complete summarization model\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_dim=300, hidden_dim=512, num_layers=2, dropout=0.3, embedding_matrix=None):\n",
    "        super(SummarizationModel, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size, embed_dim, hidden_dim, num_layers, dropout, embedding_matrix)\n",
    "        self.decoder = Decoder(vocab_size, embed_dim, hidden_dim, 1, dropout, embedding_matrix)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, article_input_ids, article_attention_mask, summary_input_ids=None, teacher_forcing_ratio=0.5):\n",
    "        batch_size = article_input_ids.size(0)\n",
    "        \n",
    "        # Encode article\n",
    "        encoder_outputs, encoder_hidden = self.encoder(article_input_ids, article_attention_mask)\n",
    "        \n",
    "        # Initialize decoder hidden state\n",
    "        decoder_hidden = encoder_hidden[-2:].mean(dim=0, keepdim=True)\n",
    "        \n",
    "        if summary_input_ids is not None:\n",
    "            # Training mode\n",
    "            max_length = summary_input_ids.size(1) - 1  # Exclude last token\n",
    "            outputs = []\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                if t == 0 or torch.rand(1).item() < teacher_forcing_ratio:\n",
    "                    # Use teacher forcing\n",
    "                    input_token = summary_input_ids[:, t:t+1]\n",
    "                else:\n",
    "                    # Use previous prediction\n",
    "                    input_token = torch.argmax(outputs[-1], dim=1, keepdim=True)\n",
    "                \n",
    "                output, decoder_hidden, _ = self.decoder(\n",
    "                    input_token, decoder_hidden, encoder_outputs, \n",
    "                    encoder_mask=article_attention_mask\n",
    "                )\n",
    "                outputs.append(output)\n",
    "            \n",
    "            return torch.stack(outputs, dim=1)\n",
    "        else:\n",
    "            # Inference mode\n",
    "            max_length = 100\n",
    "            outputs = []\n",
    "            input_token = torch.tensor([[2]], device=article_input_ids.device).repeat(batch_size, 1)  # [SOS]\n",
    "            \n",
    "            for t in range(max_length):\n",
    "                output, decoder_hidden, _ = self.decoder(\n",
    "                    input_token, decoder_hidden, encoder_outputs,\n",
    "                    encoder_mask=article_attention_mask\n",
    "                )\n",
    "                outputs.append(output)\n",
    "                input_token = torch.argmax(output, dim=1, keepdim=True)\n",
    "                \n",
    "                # Stop if all sequences have generated [EOS]\n",
    "                if (input_token == 3).all():\n",
    "                    break\n",
    "            \n",
    "            return torch.stack(outputs, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fba821",
   "metadata": {},
   "source": [
    "**MODEL TRAINING PROCESS DEFINITION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27085004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        article_input_ids = batch['article_input_ids'].to(device)\n",
    "        article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "        summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "        \n",
    "        outputs = model(article_input_ids, article_attention_mask, summary_input_ids)\n",
    "        \n",
    "        # Get target sequence excluding the last token and SOS token\n",
    "        target = summary_input_ids[:, 1:-1].contiguous()\n",
    "        \n",
    "        # Ensure outputs match target sequence length\n",
    "        outputs = outputs[:, :target.size(1), :].contiguous()\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        outputs = outputs.view(-1, outputs.shape[-1])\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device, tokenizer_wrapper):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            article_input_ids = batch['article_input_ids'].to(device)\n",
    "            article_attention_mask = batch['article_attention_mask'].to(device)\n",
    "            summary_input_ids = batch['summary_input_ids'].to(device)\n",
    "            \n",
    "            outputs = model(article_input_ids, article_attention_mask, summary_input_ids,\n",
    "                          teacher_forcing_ratio=0.0)\n",
    "            \n",
    "            # Get target sequence excluding the last token and SOS token\n",
    "            target = summary_input_ids[:, 1:-1].contiguous()\n",
    "            \n",
    "            # Ensure outputs match target sequence length\n",
    "            outputs = outputs[:, :target.size(1), :].contiguous()\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            outputs_flat = outputs.view(-1, outputs.shape[-1])\n",
    "            target_flat = target.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Calculate ROUGE scores\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            for pred, target in zip(predictions, summary_input_ids):\n",
    "                pred_text = tokenizer_wrapper.sequence_to_text(pred)\n",
    "                target_text = tokenizer_wrapper.sequence_to_text(target)\n",
    "                \n",
    "                if pred_text.strip() and target_text.strip():\n",
    "                    scores = scorer.score(target_text, pred_text)\n",
    "                    for metric in rouge_scores:\n",
    "                        rouge_scores[metric].append(scores[metric].fmeasure)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    avg_rouge_scores = {k: sum(v)/len(v) if v else 0.0 for k, v in rouge_scores.items()}\n",
    "    \n",
    "    return avg_loss, avg_rouge_scores\n",
    "\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, tokenizer_wrapper, \n",
    "                num_epochs=10, early_stopping_patience=3):\n",
    "    model.to(device)\n",
    "    best_val_loss = float('inf')\n",
    "    early_stop_counter = 0\n",
    "    training_stats = []\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, rouge_scores = evaluate(model, val_loader, criterion, device, tokenizer_wrapper)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(\"ROUGE Scores:\", rouge_scores)\n",
    "        \n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'rouge_scores': rouge_scores\n",
    "        })\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            early_stop_counter = 0\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), 'best_summarizer_model.pth')\n",
    "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "            print(f\"Early stopping counter: {early_stop_counter}/{early_stopping_patience}\")\n",
    "            \n",
    "        if early_stop_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model before returning\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Restored best model state before returning\")\n",
    "    \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bf13083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Word2Vec embeddings from gensim-data...\n",
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
      "Word2Vec embedding dimension: 300\n",
      "Building vocabulary...\n",
      "Built vocabulary with 10000 tokens\n",
      "Words from Word2Vec: 9996\n",
      "Creating embedding matrix...\n",
      "Found Word2Vec embeddings for 9996/9996 words\n"
     ]
    }
   ],
   "source": [
    "word2vec_model, embedding_dim = load_word2vec_embeddings()#('../data/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# Initialize BERT tokenizer wrapper\n",
    "print(\"Building vocabulary...\")\n",
    "tokenizer_wrapper = BertTokenizerWrapper(max_vocab_size=10000)\n",
    "all_texts = train_df['article'].tolist() + train_df['summary'].tolist()\n",
    "tokenizer_wrapper.build_vocabulary_from_word2vec(word2vec_model, all_texts)\n",
    "\n",
    "# Create Word2Vec embeddings for the vocabulary\n",
    "print(\"Creating embedding matrix...\")\n",
    "word2vec_embeddings = Word2VecEmbeddings(word2vec_model, embedding_dim)\n",
    "embedding_matrix = word2vec_embeddings.create_embedding_matrix(tokenizer_wrapper)\n",
    "\n",
    "train_dataset = SummarizationDataset(\n",
    "        train_df['article'].tolist(), \n",
    "        train_df['summary'].tolist(), \n",
    "        tokenizer_wrapper\n",
    "    )\n",
    "val_dataset = SummarizationDataset(\n",
    "    val_df['article'].tolist(), \n",
    "    val_df['summary'].tolist(), \n",
    "    tokenizer_wrapper\n",
    "    )\n",
    "test_dataset = SummarizationDataset(\n",
    "    test_df['article'].tolist(), \n",
    "    test_df['summary'].tolist(), \n",
    "    tokenizer_wrapper\n",
    "    )\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eacb6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ud/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [07:07<00:00,  2.92it/s]\n",
      "Evaluating: 100%|██████████| 125/125 [00:22<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.2986\n",
      "Val Loss: 5.3650\n",
      "ROUGE Scores: {'rouge1': 0.21370058618501728, 'rouge2': 0.05866255322628348, 'rougeL': 0.2121067222012737}\n",
      "Saved new best model with validation loss: 5.3650\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [07:11<00:00,  2.90it/s]\n",
      "Evaluating: 100%|██████████| 125/125 [00:23<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8008\n",
      "Val Loss: 5.2590\n",
      "ROUGE Scores: {'rouge1': 0.2155998810540827, 'rouge2': 0.059726249492114004, 'rougeL': 0.21317007075988284}\n",
      "Saved new best model with validation loss: 5.2590\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1250/1250 [07:09<00:00,  2.91it/s]\n",
      "Evaluating: 100%|██████████| 125/125 [00:23<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.5194\n",
      "Val Loss: 5.2439\n",
      "ROUGE Scores: {'rouge1': 0.21900713264397223, 'rouge2': 0.06182522782618901, 'rougeL': 0.2149890894767001}\n",
      "Saved new best model with validation loss: 5.2439\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 277/1250 [01:34<05:30,  2.94it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m trained_model, training_stats = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_patience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating on test set...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 90\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, device, tokenizer_wrapper, num_epochs, early_stopping_patience)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     val_loss, rouge_scores = evaluate(model, val_loader, criterion, device, tokenizer_wrapper)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device, clip)\u001b[39m\n\u001b[32m      9\u001b[39m article_attention_mask = batch[\u001b[33m'\u001b[39m\u001b[33marticle_attention_mask\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     10\u001b[39m summary_input_ids = batch[\u001b[33m'\u001b[39m\u001b[33msummary_input_ids\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marticle_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msummary_input_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Get target sequence excluding the last token and SOS token\u001b[39;00m\n\u001b[32m     15\u001b[39m target = summary_input_ids[:, \u001b[32m1\u001b[39m:-\u001b[32m1\u001b[39m].contiguous()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 134\u001b[39m, in \u001b[36mSummarizationModel.forward\u001b[39m\u001b[34m(self, article_input_ids, article_attention_mask, summary_input_ids, teacher_forcing_ratio)\u001b[39m\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    131\u001b[39m         \u001b[38;5;66;03m# Use previous prediction\u001b[39;00m\n\u001b[32m    132\u001b[39m         input_token = torch.argmax(outputs[-\u001b[32m1\u001b[39m], dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     output, decoder_hidden, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43marticle_attention_mask\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     outputs.append(output)\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch.stack(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mDecoder.forward\u001b[39m\u001b[34m(self, input_token, decoder_hidden, encoder_outputs, encoder_mask)\u001b[39m\n\u001b[32m     92\u001b[39m context = context.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     93\u001b[39m gru_input = torch.cat([embedded, context], dim=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m output, decoder_hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgru_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# Final prediction\u001b[39;00m\n\u001b[32m     98\u001b[39m prediction_input = torch.cat([output.squeeze(\u001b[32m1\u001b[39m), context.squeeze(\u001b[32m1\u001b[39m)], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/tsm/TextSummarizationModel/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1393\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1393\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1405\u001b[39m     result = _VF.gru(\n\u001b[32m   1406\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1407\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1414\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1415\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Initializing model...\")\n",
    "vocab_size = len(tokenizer_wrapper.vocab_to_int)\n",
    "model = SummarizationModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embedding_dim,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    "    embedding_matrix=embedding_matrix\n",
    ")\n",
    "\n",
    "# Initialize optimizer and criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trained_model, training_stats = train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, device, tokenizer_wrapper,\n",
    "    num_epochs=3, early_stopping_patience=3\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\")\n",
    "test_loss, test_rouge_scores = evaluate(trained_model, test_loader, criterion, device, tokenizer_wrapper)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"Test ROUGE Scores:\", test_rouge_scores)\n",
    "\n",
    "# Save final model and tokenizer\n",
    "torch.save({\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'vocab_size': vocab_size,\n",
    "    'training_stats': training_stats,\n",
    "    'test_scores': {'loss': test_loss, 'rouge': test_rouge_scores}\n",
    "}, 'final_summarization_model.pth')\n",
    "\n",
    "with open('tokenizer_wrapper.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer_wrapper, f)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Example inference\n",
    "print(\"\\nExample inference:\")\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_idx = 0\n",
    "    sample_article = test_df.iloc[sample_idx]['article']\n",
    "    actual_summary = test_df.iloc[sample_idx]['summary']\n",
    "    \n",
    "    # Preprocess\n",
    "    article_seq = tokenizer_wrapper.text_to_sequence(sample_article)\n",
    "    article_tensor = torch.tensor([article_seq], dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones_like(article_tensor).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    outputs = trained_model(article_tensor, attention_mask)\n",
    "    predicted_ids = torch.argmax(outputs, dim=-1)[0]\n",
    "    \n",
    "    # Decode\n",
    "    generated_summary = tokenizer_wrapper.sequence_to_text(predicted_ids)\n",
    "    \n",
    "    print(f\"Article: {sample_article[:200]}...\")\n",
    "    print(f\"Actual Summary: {actual_summary}\")\n",
    "    print(f\"Generated Summary: {generated_summary}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
